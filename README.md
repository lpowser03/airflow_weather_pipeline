# Weather Prediction Data Pipeline with Airflow
This project implements a data pipeline using Apache Airflow to periodically fetch weather data for San Francisco from the OpenWeatherMap API, transform it, and store it in a PostgreSQL database. The entire environment is containerized using Docker for easy setup and deployment.
## Features
- **Automated Data Collection**: The Airflow DAG is scheduled to run daily.
- **ETL Process**:
    - **Extract**: Fetches current weather data from the OpenWeatherMap API.
    - **Transform**: Converts temperature from Kelvin to Fahrenheit and cleans the data.
    - **Load**: Inserts the processed data into a PostgreSQL database.

- **Containerized**: Uses Docker and Docker Compose to manage all services (Airflow, Postgres, etc.).
- **Scalable**: Built with Airflow, allowing for easy extension to include more data sources or complex transformations.

## Architecture
The project consists of three main components orchestrated by Docker Compose:
1. **Apache Airflow**: The workflow management system used to schedule, orchestrate, and monitor the data pipeline. This includes the Airflow webserver, scheduler, and worker.
2. **PostgreSQL**: The database used as both the Airflow backend and the destination for the collected weather data.
3. **ETL DAG (`weather_pipeline`)**: The Python script defining the pipeline's tasks and dependencies.

## Prerequisites
- Docker
- Docker Compose

## Getting Started
1. **Clone the repository:**
```shell
    git clone <your-repository-url>
    cd weather_prediction_airflow
```
2. **Create an environment file:** Create a file in the project root. You can use the `.env.example` as a template. This file will store your credentials. `.env`
```  
    API_KEY="your_openweathermap_api_key"
    HOST="postgres"
    POSTGRES_DB="airflow"
    POSTGRES_USER="airflow"
    POSTGRES_PASSWORD="airflow"
```
- Get your `API_KEY` from [OpenWeatherMap](https://openweathermap.org/api).
- The other variables should match the values used in your `docker-compose.yml` for the PostgreSQL service.
3. Build and run the containers:
`
    docker-compose up --build -d
`
This will start the Airflow and PostgresSQL services in the background.
4. **Access the Airflow UI:** Open your web browser and navigate to `http://localhost:8080`.

## Important: Airflow Admin User
The initial Airflow admin user's password generated by the default setup can be difficult to find in the logs. It is recommended to create a new admin user manually.
1. **Exec into the Airflow webserver container:**
`
    docker-compose exec airflow-webserver bash
`
2. Delete auto-created admin w/ unknown passcode and create a new admin user (replace placeholders with your details):
```shell
    airflow users delete --username admin
    airflow users create \
        --username <your_username> \
        --firstname <your_firstname> \
        --lastname <your_lastname> \
        --role Admin \
        --email <your_email@example.com> \
        --password <your_password>
```
3. You can now log in to the Airflow UI with your new credentials

## Database Schema
The weather data is stored in the table, which is created using the following SQL schema: `weather_data`
```sql
CREATE TABLE weather_data (
    id SERIAL PRIMARY KEY,
    collected_at TIMESTAMP UNIQUE NOT NULL,
    temperature DECIMAL(5,2),
    humidity INTEGER,
    pressure DECIMAL(7,2),
    wind_speed DECIMAL(5,2),
    weather_condition VARCHAR(50),
    visibility INTEGER,
    city VARCHAR(50) DEFAULT 'San Francisco'
);
```

## Project Structure
```
    .
    ├── config/               # Airflow configuration files
    ├── dags/                 # Airflow DAG scripts
    │   └── weather_pipeline.py
    ├── logs/                 # Logs from Airflow tasks
    ├── sql/                  # SQL scripts
    │   └── create_tables.sql
    ├── .env                  # Environment variables (needs to be created)
    ├── docker-compose.yml    # Docker Compose configuration
    └── requirements.txt      # Python dependencies
```
